"D:\CS4341 Intro to Artificial Intelligence\python projects\venv\Scripts\python.exe" "D:/CS4341 Intro to Artificial Intelligence/project1/Project_2/template.py"
Using TensorFlow backend.
Train on 3895 samples, validate on 978 samples
Epoch 1/20
2018-02-24 12:41:48.788614: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2

 512/3895 [==>...........................] - ETA: 1s - loss: 9.1441 - acc: 0.1191
3895/3895 [==============================] - 0s 68us/step - loss: 3.8571 - acc: 0.1122 - val_loss: 2.3075 - val_acc: 0.1125
Epoch 2/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3029 - acc: 0.1133
3895/3895 [==============================] - 0s 12us/step - loss: 2.3027 - acc: 0.1117 - val_loss: 2.3065 - val_acc: 0.1125
Epoch 3/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3012 - acc: 0.1211
3895/3895 [==============================] - 0s 13us/step - loss: 2.3025 - acc: 0.1117 - val_loss: 2.3064 - val_acc: 0.1125
Epoch 4/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3004 - acc: 0.0781
3895/3895 [==============================] - 0s 15us/step - loss: 2.3023 - acc: 0.1117 - val_loss: 2.3059 - val_acc: 0.1125
Epoch 5/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3040 - acc: 0.0938
3895/3895 [==============================] - 0s 14us/step - loss: 2.3022 - acc: 0.1122 - val_loss: 2.3068 - val_acc: 0.1125
Epoch 6/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3017 - acc: 0.1250
3895/3895 [==============================] - 0s 13us/step - loss: 2.3021 - acc: 0.1122 - val_loss: 2.3067 - val_acc: 0.1125
Epoch 7/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3022 - acc: 0.0996
3895/3895 [==============================] - 0s 12us/step - loss: 2.3021 - acc: 0.1122 - val_loss: 2.3071 - val_acc: 0.1125
Epoch 8/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3027 - acc: 0.1094
3895/3895 [==============================] - 0s 13us/step - loss: 2.3019 - acc: 0.1122 - val_loss: 2.3074 - val_acc: 0.1125
Epoch 9/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3025 - acc: 0.1113
3895/3895 [==============================] - 0s 13us/step - loss: 2.3018 - acc: 0.1122 - val_loss: 2.3077 - val_acc: 0.1125
Epoch 10/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3004 - acc: 0.1074
3895/3895 [==============================] - 0s 13us/step - loss: 2.3017 - acc: 0.1122 - val_loss: 2.3076 - val_acc: 0.1125
Epoch 11/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3008 - acc: 0.1055
3895/3895 [==============================] - 0s 13us/step - loss: 2.3016 - acc: 0.1125 - val_loss: 2.3078 - val_acc: 0.1125
Epoch 12/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3019 - acc: 0.1016
3895/3895 [==============================] - 0s 13us/step - loss: 2.3013 - acc: 0.1125 - val_loss: 2.3081 - val_acc: 0.1125
Epoch 13/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3022 - acc: 0.1055
3895/3895 [==============================] - 0s 13us/step - loss: 2.3011 - acc: 0.1127 - val_loss: 2.3090 - val_acc: 0.1125
Epoch 14/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3010 - acc: 0.1074
3895/3895 [==============================] - 0s 13us/step - loss: 2.3006 - acc: 0.1122 - val_loss: 2.3093 - val_acc: 0.1125
Epoch 15/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2993 - acc: 0.1172
3895/3895 [==============================] - 0s 13us/step - loss: 2.2999 - acc: 0.1125 - val_loss: 2.3099 - val_acc: 0.1125
Epoch 16/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3014 - acc: 0.1211
3895/3895 [==============================] - 0s 12us/step - loss: 2.2977 - acc: 0.1125 - val_loss: 2.3069 - val_acc: 0.1135
Epoch 17/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2991 - acc: 0.1074
3895/3895 [==============================] - 0s 13us/step - loss: 2.2790 - acc: 0.1091 - val_loss: 2.2835 - val_acc: 0.1145
Epoch 18/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2613 - acc: 0.1152
3895/3895 [==============================] - 0s 14us/step - loss: 2.2499 - acc: 0.1122 - val_loss: 2.2412 - val_acc: 0.1656
Epoch 19/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2354 - acc: 0.1406
3895/3895 [==============================] - 0s 13us/step - loss: 2.2191 - acc: 0.1807 - val_loss: 2.2054 - val_acc: 0.1933
Epoch 20/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1724 - acc: 0.2031
3895/3895 [==============================] - 0s 14us/step - loss: 2.1876 - acc: 0.1913 - val_loss: 2.1809 - val_acc: 0.1963
{'val_loss': [2.3074875584896843, 2.3065019848400343, 2.306357332533854, 2.305909083664783, 2.306787086166975, 2.306652262166965, 2.307124045241342, 2.3074191123673766, 2.307727989731147, 2.3076461921684093, 2.3078413341186774, 2.308148947230146, 2.3090222822864117, 2.3093134454910498, 2.309865007127477, 2.3068773059025864, 2.2835029414826375, 2.241158723343613, 2.2053958097118542, 2.1809241664434014], 'val_acc': [0.11247443762781185, 0.11247443762781185, 0.11247443762781185, 0.11247443762781185, 0.11247443762781185, 0.11247443762781185, 0.11247443762781185, 0.11247443762781185, 0.11247443762781185, 0.11247443762781185, 0.11247443762781185, 0.11247443762781185, 0.11247443762781185, 0.11247443762781185, 0.11247443762781185, 0.11349693251343287, 0.11451942739714935, 0.16564417177342747, 0.19325153374233128, 0.19631901840300342], 'loss': [3.857081589925274, 2.3026692413702246, 2.3024710938621396, 2.302347422870347, 2.3022193018676993, 2.3021035976679243, 2.302055625217717, 2.3019131763296654, 2.301822996384372, 2.3017005562935315, 2.3015601809431865, 2.3013432774525397, 2.3010896914730634, 2.300604966914088, 2.299873036628204, 2.297712121909635, 2.2789531748163228, 2.2498869017550818, 2.219102331058664, 2.1875635200348684], 'acc': [0.11219512211955099, 0.11168164341914945, 0.11168164285868215, 0.11168164330055227, 0.11219512210233527, 0.11219512223814816, 0.11219512193209094, 0.11219512177906232, 0.11219512181349377, 0.11219512194930666, 0.11245186164764752, 0.11245186176624469, 0.11270860102271545, 0.11219512223814816, 0.11245186164764752, 0.11245186135880601, 0.10911424878090428, 0.11219512250977395, 0.1807445437060394, 0.19127085968297314]}
