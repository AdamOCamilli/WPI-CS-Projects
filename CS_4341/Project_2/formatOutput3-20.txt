"D:\CS4341 Intro to Artificial Intelligence\python projects\venv\Scripts\python.exe" "D:/CS4341 Intro to Artificial Intelligence/project1/Project_2/template.py"
Using TensorFlow backend.
Train on 3895 samples, validate on 978 samples
Epoch 1/20
2018-02-24 12:26:51.498818: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2

 512/3895 [==>...........................] - ETA: 1s - loss: 2.3031 - acc: 0.1270
3895/3895 [==============================] - 0s 83us/step - loss: 2.3033 - acc: 0.1060 - val_loss: 2.3027 - val_acc: 0.1053
Epoch 2/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3036 - acc: 0.0918
3895/3895 [==============================] - 0s 14us/step - loss: 2.3027 - acc: 0.1076 - val_loss: 2.3022 - val_acc: 0.1094
Epoch 3/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3031 - acc: 0.1113
3895/3895 [==============================] - 0s 13us/step - loss: 2.3022 - acc: 0.1071 - val_loss: 2.3017 - val_acc: 0.1094
Epoch 4/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3024 - acc: 0.0918
3895/3895 [==============================] - 0s 14us/step - loss: 2.3018 - acc: 0.1101 - val_loss: 2.3013 - val_acc: 0.1115
Epoch 5/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3012 - acc: 0.1191
3895/3895 [==============================] - 0s 14us/step - loss: 2.3014 - acc: 0.1158 - val_loss: 2.3009 - val_acc: 0.1125
Epoch 6/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3022 - acc: 0.0820
3895/3895 [==============================] - 0s 14us/step - loss: 2.3010 - acc: 0.1148 - val_loss: 2.3005 - val_acc: 0.1155
Epoch 7/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3009 - acc: 0.1172
3895/3895 [==============================] - 0s 14us/step - loss: 2.3006 - acc: 0.1160 - val_loss: 2.3001 - val_acc: 0.1166
Epoch 8/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3004 - acc: 0.1191
3895/3895 [==============================] - 0s 14us/step - loss: 2.3002 - acc: 0.1181 - val_loss: 2.2996 - val_acc: 0.1217
Epoch 9/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2995 - acc: 0.1211
3895/3895 [==============================] - 0s 14us/step - loss: 2.2997 - acc: 0.1225 - val_loss: 2.2990 - val_acc: 0.1247
Epoch 10/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2980 - acc: 0.1250
3895/3895 [==============================] - 0s 14us/step - loss: 2.2992 - acc: 0.1237 - val_loss: 2.2984 - val_acc: 0.1258
Epoch 11/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2976 - acc: 0.1309
3895/3895 [==============================] - 0s 14us/step - loss: 2.2985 - acc: 0.1261 - val_loss: 2.2976 - val_acc: 0.1258
Epoch 12/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2983 - acc: 0.1328
3895/3895 [==============================] - 0s 14us/step - loss: 2.2978 - acc: 0.1289 - val_loss: 2.2967 - val_acc: 0.1288
Epoch 13/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2949 - acc: 0.1367
3895/3895 [==============================] - 0s 14us/step - loss: 2.2968 - acc: 0.1273 - val_loss: 2.2955 - val_acc: 0.1278
Epoch 14/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2959 - acc: 0.1191
3895/3895 [==============================] - 0s 15us/step - loss: 2.2955 - acc: 0.1297 - val_loss: 2.2939 - val_acc: 0.1268
Epoch 15/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2950 - acc: 0.1230
3895/3895 [==============================] - 0s 13us/step - loss: 2.2938 - acc: 0.1322 - val_loss: 2.2916 - val_acc: 0.1258
Epoch 16/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2923 - acc: 0.1367
3895/3895 [==============================] - 0s 15us/step - loss: 2.2914 - acc: 0.1320 - val_loss: 2.2885 - val_acc: 0.1288
Epoch 17/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2857 - acc: 0.1465
3895/3895 [==============================] - 0s 14us/step - loss: 2.2880 - acc: 0.1361 - val_loss: 2.2840 - val_acc: 0.1350
Epoch 18/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2824 - acc: 0.1211
3895/3895 [==============================] - 0s 15us/step - loss: 2.2831 - acc: 0.1371 - val_loss: 2.2775 - val_acc: 0.1391
Epoch 19/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2833 - acc: 0.1289
3895/3895 [==============================] - 0s 13us/step - loss: 2.2762 - acc: 0.1392 - val_loss: 2.2688 - val_acc: 0.1421
Epoch 20/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2699 - acc: 0.1309
3895/3895 [==============================] - 0s 14us/step - loss: 2.2671 - acc: 0.1463 - val_loss: 2.2570 - val_acc: 0.1483
{'val_loss': [2.302689045728594, 2.3021633712791956, 2.301714524169641, 2.3013017470119923, 2.3009002881547427, 2.3004914773266254, 2.3000614847873617, 2.2995816062076937, 2.2990336432778764, 2.298408455643917, 2.2976420301113634, 2.296682496256136, 2.295476859820157, 2.2938889730683623, 2.2916429018437983, 2.2885208334659506, 2.2839847747046034, 2.277539484339989, 2.2688215559001836, 2.2570396903102385], 'val_acc': [0.10531697588342342, 0.109406952325308, 0.109406952325308, 0.1114519452085768, 0.11247444009610237, 0.11554192475867905, 0.11656441964620462, 0.12167689408383242, 0.12474437874640912, 0.12576687363393468, 0.12576687363393468, 0.12883435829651135, 0.1278118634089858, 0.12678936763775128, 0.12576686875829912, 0.1288343534208758, 0.13496932673795578, 0.13905930717176698, 0.14212679095063474, 0.14826175716757042], 'loss': [2.303295409908956, 2.3027052671214583, 2.3022301201948916, 2.301800928862624, 2.301399239465422, 2.301013988463043, 2.300617886324137, 2.3001972606154553, 2.2997198060174044, 2.2991795040370566, 2.2985401616935093, 2.297765018330918, 2.296785249208149, 2.2954994680945773, 2.293815702590159, 2.2914005537853313, 2.288026415796733, 2.2831052412270583, 2.276226635860693, 2.2671141038657154], 'acc': [0.10603337627435067, 0.10757381286715971, 0.1070603337765352, 0.11014120665609607, 0.11578947366508195, 0.11476251599265308, 0.11604621331177566, 0.11810012807895039, 0.12246469819346809, 0.12374839552980639, 0.12605905046971067, 0.12888318309046337, 0.12734274728001313, 0.1296534016422344, 0.13222079629769526, 0.1319640564635415, 0.13607188728715702, 0.13709884477212586, 0.13915276009976787, 0.14634146363269993]}
