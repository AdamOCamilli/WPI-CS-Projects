"D:\CS4341 Intro to Artificial Intelligence\python projects\venv\Scripts\python.exe" "D:/CS4341 Intro to Artificial Intelligence/project1/Project_2/template.py"
Using TensorFlow backend.
Train on 3895 samples, validate on 978 samples
Epoch 1/25
2018-02-26 11:18:27.657185: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2

 512/3895 [==>...........................] - ETA: 1s - loss: 7.5507 - acc: 0.1016
3895/3895 [==============================] - 0s 77us/step - loss: 2.9569 - acc: 0.1669 - val_loss: 2.2082 - val_acc: 0.2178
Epoch 2/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2124 - acc: 0.2285
3895/3895 [==============================] - 0s 14us/step - loss: 2.1543 - acc: 0.2447 - val_loss: 2.0860 - val_acc: 0.2393
Epoch 3/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.9893 - acc: 0.2832
3895/3895 [==============================] - 0s 14us/step - loss: 2.0214 - acc: 0.2591 - val_loss: 1.9765 - val_acc: 0.2761
Epoch 4/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.9980 - acc: 0.2480
3895/3895 [==============================] - 0s 13us/step - loss: 1.9367 - acc: 0.2834 - val_loss: 1.9306 - val_acc: 0.2894
Epoch 5/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.9350 - acc: 0.3223
3895/3895 [==============================] - 0s 16us/step - loss: 1.8938 - acc: 0.3096 - val_loss: 1.9033 - val_acc: 0.2945
Epoch 6/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.8877 - acc: 0.3066
3895/3895 [==============================] - 0s 16us/step - loss: 1.8483 - acc: 0.3245 - val_loss: 1.8582 - val_acc: 0.3313
Epoch 7/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.8508 - acc: 0.2910
3895/3895 [==============================] - 0s 16us/step - loss: 1.8421 - acc: 0.3245 - val_loss: 1.8082 - val_acc: 0.3354
Epoch 8/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.7878 - acc: 0.3516
3895/3895 [==============================] - 0s 14us/step - loss: 1.7841 - acc: 0.3422 - val_loss: 1.8390 - val_acc: 0.3558
Epoch 9/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.8211 - acc: 0.3301
3895/3895 [==============================] - 0s 14us/step - loss: 1.7439 - acc: 0.3569 - val_loss: 1.7527 - val_acc: 0.3620
Epoch 10/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.6914 - acc: 0.3730
3895/3895 [==============================] - 0s 16us/step - loss: 1.7032 - acc: 0.3712 - val_loss: 1.7375 - val_acc: 0.3671
Epoch 11/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.6687 - acc: 0.3789
3895/3895 [==============================] - 0s 16us/step - loss: 1.6737 - acc: 0.3772 - val_loss: 1.7158 - val_acc: 0.3763
Epoch 12/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.6861 - acc: 0.3730
3895/3895 [==============================] - 0s 14us/step - loss: 1.6651 - acc: 0.3849 - val_loss: 1.7513 - val_acc: 0.3814
Epoch 13/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.6451 - acc: 0.4355
3895/3895 [==============================] - 0s 14us/step - loss: 1.6492 - acc: 0.3908 - val_loss: 1.6940 - val_acc: 0.3732
Epoch 14/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.5846 - acc: 0.3906
3895/3895 [==============================] - 0s 13us/step - loss: 1.6066 - acc: 0.3967 - val_loss: 1.6636 - val_acc: 0.3793
Epoch 15/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.6235 - acc: 0.3965
3895/3895 [==============================] - 0s 13us/step - loss: 1.5895 - acc: 0.4031 - val_loss: 1.6631 - val_acc: 0.4029
Epoch 16/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.5551 - acc: 0.4219
3895/3895 [==============================] - 0s 15us/step - loss: 1.5759 - acc: 0.4031 - val_loss: 1.6582 - val_acc: 0.4387
Epoch 17/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.5947 - acc: 0.4316
3895/3895 [==============================] - 0s 16us/step - loss: 1.5558 - acc: 0.4398 - val_loss: 1.6459 - val_acc: 0.4213
Epoch 18/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.6224 - acc: 0.4082
3895/3895 [==============================] - 0s 14us/step - loss: 1.5569 - acc: 0.4375 - val_loss: 1.6168 - val_acc: 0.4387
Epoch 19/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.5675 - acc: 0.4590
3895/3895 [==============================] - 0s 13us/step - loss: 1.5331 - acc: 0.4496 - val_loss: 1.6345 - val_acc: 0.4387
Epoch 20/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.4839 - acc: 0.4805
3895/3895 [==============================] - 0s 13us/step - loss: 1.5283 - acc: 0.4531 - val_loss: 1.6208 - val_acc: 0.4407
Epoch 21/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.5010 - acc: 0.4590
3895/3895 [==============================] - 0s 14us/step - loss: 1.5097 - acc: 0.4531 - val_loss: 1.6293 - val_acc: 0.4479
Epoch 22/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.5116 - acc: 0.4961
3895/3895 [==============================] - 0s 14us/step - loss: 1.4890 - acc: 0.4596 - val_loss: 1.6125 - val_acc: 0.4438
Epoch 23/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.4956 - acc: 0.4473
3895/3895 [==============================] - 0s 15us/step - loss: 1.4843 - acc: 0.4544 - val_loss: 1.5975 - val_acc: 0.4458
Epoch 24/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.5047 - acc: 0.4570
3895/3895 [==============================] - 0s 15us/step - loss: 1.4749 - acc: 0.4647 - val_loss: 1.6270 - val_acc: 0.4448
Epoch 25/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.4487 - acc: 0.4746
3895/3895 [==============================] - 0s 14us/step - loss: 1.4630 - acc: 0.4696 - val_loss: 1.5729 - val_acc: 0.4581
{'val_loss': [2.208184700558278, 2.0860218285051593, 1.9765044165535208, 1.9305611797637003, 1.903334946232583, 1.8582272929404167, 1.8081914927574267, 1.8390421594335014, 1.7527370499199397, 1.737496768038697, 1.7157927890496751, 1.7513085798739412, 1.6940026249134712, 1.6636336239812808, 1.663051988205783, 1.6581606899058647, 1.6458976251703585, 1.6168172734890491, 1.6344557883549322, 1.6207861832070691, 1.6292705523699582, 1.6124684832578788, 1.597466030491398, 1.627040360115301, 1.572940345678349], 'val_acc': [0.2177914110315175, 0.2392638048998905, 0.27607361771212036, 0.289366051646098, 0.2944785291614708, 0.33128834727595435, 0.3353783254852568, 0.35582822585642215, 0.3619631889651402, 0.3670756624885863, 0.3762781243992242, 0.3813905979836158, 0.3732106432105378, 0.37934559388638517, 0.40286298099227236, 0.4386503043715939, 0.4212678953060587, 0.4386503057123937, 0.43865030882061135, 0.4406952963711538, 0.447852758359324, 0.44376277777314915, 0.4458077734599084, 0.44478527458045625, 0.45807770400447106], 'loss': [2.956916071851385, 2.15427132075191, 2.021382209119197, 1.9366696542585615, 1.893833222376979, 1.8483085018985537, 1.842078954968434, 1.7840882083111764, 1.7438838226039235, 1.7032296746012794, 1.6736556444302757, 1.6650502200610218, 1.649179531123121, 1.606594655510703, 1.5895287353664982, 1.575930729573437, 1.555830507223352, 1.5568530108411445, 1.5331489241628193, 1.52829248761335, 1.5096667713929202, 1.4890210133614987, 1.484302757640246, 1.4748963772255894, 1.4630294825819552], 'acc': [0.16688061651124575, 0.24467265713046543, 0.2590500636875094, 0.2834403084392572, 0.30962772866727756, 0.32451861443354324, 0.3245186135689315, 0.3422336319903813, 0.356867778959262, 0.3712451850419173, 0.37715019176645975, 0.38485237566589237, 0.39075738116620595, 0.3966623867353824, 0.40308087397754117, 0.4030808727533123, 0.43979460769960604, 0.4374839549346209, 0.44955070610989045, 0.4531450576898528, 0.4531450577587157, 0.45956354314922826, 0.4544287542629609, 0.46469833218852424, 0.46957638112204064]}
