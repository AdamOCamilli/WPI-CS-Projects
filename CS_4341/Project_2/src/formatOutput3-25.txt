"D:\CS4341 Intro to Artificial Intelligence\python projects\venv\Scripts\python.exe" "D:/CS4341 Intro to Artificial Intelligence/project1/Project_2/template.py"
Using TensorFlow backend.
Train on 3895 samples, validate on 978 samples
Epoch 1/25
2018-02-24 12:27:54.760129: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2

 512/3895 [==>...........................] - ETA: 1s - loss: 2.3038 - acc: 0.0625
3895/3895 [==============================] - 0s 85us/step - loss: 2.3032 - acc: 0.0773 - val_loss: 2.3028 - val_acc: 0.0920
Epoch 2/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3032 - acc: 0.0898
3895/3895 [==============================] - 0s 14us/step - loss: 2.3025 - acc: 0.0888 - val_loss: 2.3022 - val_acc: 0.0920
Epoch 3/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3015 - acc: 0.0801
3895/3895 [==============================] - 0s 14us/step - loss: 2.3019 - acc: 0.0963 - val_loss: 2.3016 - val_acc: 0.0941
Epoch 4/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3010 - acc: 0.1016
3895/3895 [==============================] - 0s 14us/step - loss: 2.3013 - acc: 0.0991 - val_loss: 2.3010 - val_acc: 0.1002
Epoch 5/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3008 - acc: 0.1035
3895/3895 [==============================] - 0s 15us/step - loss: 2.3007 - acc: 0.1017 - val_loss: 2.3003 - val_acc: 0.1063
Epoch 6/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3009 - acc: 0.1172
3895/3895 [==============================] - 0s 14us/step - loss: 2.3000 - acc: 0.1078 - val_loss: 2.2996 - val_acc: 0.1053
Epoch 7/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3006 - acc: 0.0996
3895/3895 [==============================] - 0s 14us/step - loss: 2.2991 - acc: 0.1055 - val_loss: 2.2986 - val_acc: 0.1012
Epoch 8/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3008 - acc: 0.0820
3895/3895 [==============================] - 0s 14us/step - loss: 2.2980 - acc: 0.1017 - val_loss: 2.2974 - val_acc: 0.1022
Epoch 9/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2998 - acc: 0.0781
3895/3895 [==============================] - 0s 15us/step - loss: 2.2967 - acc: 0.1014 - val_loss: 2.2958 - val_acc: 0.0961
Epoch 10/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2962 - acc: 0.1211
3895/3895 [==============================] - 0s 14us/step - loss: 2.2949 - acc: 0.0991 - val_loss: 2.2937 - val_acc: 0.0982
Epoch 11/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2915 - acc: 0.1074
3895/3895 [==============================] - 0s 14us/step - loss: 2.2925 - acc: 0.1009 - val_loss: 2.2909 - val_acc: 0.1022
Epoch 12/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2906 - acc: 0.1094
3895/3895 [==============================] - 0s 14us/step - loss: 2.2892 - acc: 0.1022 - val_loss: 2.2871 - val_acc: 0.1074
Epoch 13/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2871 - acc: 0.0938
3895/3895 [==============================] - 0s 15us/step - loss: 2.2849 - acc: 0.1030 - val_loss: 2.2819 - val_acc: 0.1104
Epoch 14/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2768 - acc: 0.1191
3895/3895 [==============================] - 0s 14us/step - loss: 2.2792 - acc: 0.1068 - val_loss: 2.2749 - val_acc: 0.1207
Epoch 15/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2714 - acc: 0.1133
3895/3895 [==============================] - 0s 14us/step - loss: 2.2716 - acc: 0.1127 - val_loss: 2.2663 - val_acc: 0.1288
Epoch 16/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2691 - acc: 0.1191
3895/3895 [==============================] - 0s 14us/step - loss: 2.2624 - acc: 0.1209 - val_loss: 2.2556 - val_acc: 0.1360
Epoch 17/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2575 - acc: 0.1367
3895/3895 [==============================] - 0s 14us/step - loss: 2.2514 - acc: 0.1348 - val_loss: 2.2425 - val_acc: 0.1452
Epoch 18/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2547 - acc: 0.1211
3895/3895 [==============================] - 0s 14us/step - loss: 2.2379 - acc: 0.1492 - val_loss: 2.2270 - val_acc: 0.1636
Epoch 19/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2282 - acc: 0.1816
3895/3895 [==============================] - 0s 13us/step - loss: 2.2221 - acc: 0.1661 - val_loss: 2.2096 - val_acc: 0.1789
Epoch 20/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2237 - acc: 0.1758
3895/3895 [==============================] - 0s 13us/step - loss: 2.2050 - acc: 0.1777 - val_loss: 2.1926 - val_acc: 0.1830
Epoch 21/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1918 - acc: 0.1777
3895/3895 [==============================] - 0s 14us/step - loss: 2.1878 - acc: 0.1866 - val_loss: 2.1754 - val_acc: 0.1881
Epoch 22/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1785 - acc: 0.1934
3895/3895 [==============================] - 0s 14us/step - loss: 2.1712 - acc: 0.1915 - val_loss: 2.1600 - val_acc: 0.1892
Epoch 23/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1754 - acc: 0.1895
3895/3895 [==============================] - 0s 13us/step - loss: 2.1543 - acc: 0.1951 - val_loss: 2.1453 - val_acc: 0.1953
Epoch 24/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1563 - acc: 0.2188
3895/3895 [==============================] - 0s 13us/step - loss: 2.1379 - acc: 0.1956 - val_loss: 2.1322 - val_acc: 0.1984
Epoch 25/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1058 - acc: 0.1875
3895/3895 [==============================] - 0s 14us/step - loss: 2.1231 - acc: 0.1990 - val_loss: 2.1222 - val_acc: 0.1984
{'val_loss': [2.3028356102590424, 2.302178835820079, 2.3015946342413653, 2.3009959256965935, 2.300341390637045, 2.29956965261198, 2.298606341601881, 2.2974075749364125, 2.295799223191899, 2.2937353622693957, 2.2908691667584065, 2.2870605830522157, 2.2818732481061317, 2.2749345414721405, 2.2662825404013107, 2.255590463708515, 2.2424594668522935, 2.226984470656069, 2.2096012521131394, 2.19262178885181, 2.175416154607917, 2.160011050647509, 2.14532843591733, 2.1322273264633367, 2.122207646477198], 'val_acc': [0.09202453661671933, 0.09202454325977279, 0.09406952770209752, 0.10020449967837773, 0.10633947165465794, 0.10531697588342342, 0.10122699456590328, 0.10224948634521118, 0.09611452146907526, 0.09815951124412638, 0.10224948856971991, 0.10736195989913004, 0.11042944766992441, 0.12065439831259792, 0.1288343534208758, 0.1359918207417724, 0.1451942756132114, 0.16359918358867154, 0.17893660290962835, 0.183026587792457, 0.18813905601364947, 0.18916155312568375, 0.19529652245083715, 0.19836400622970488, 0.19836400534599594], 'loss': [2.303162396642761, 2.302498840153294, 2.301907580930247, 2.301304654928166, 2.3006801824361585, 2.2999505630047543, 2.29910509766908, 2.2980374896939515, 2.296683689259442, 2.2948664037804853, 2.292495404526266, 2.2892370007922622, 2.2849130073469013, 2.2791644550563435, 2.271613727087234, 2.26242205389657, 2.2513541404640875, 2.2378648893211865, 2.222080504328051, 2.20495390518639, 2.187824433514028, 2.1712188594607547, 2.154296828662936, 2.1379148568360278, 2.123053969284077], 'acc': [0.0772785620201996, 0.0888318358551094, 0.09627727859477789, 0.0991014119290265, 0.10166880598958863, 0.1078305521580619, 0.10551989718372616, 0.10166880602402008, 0.10141206676754932, 0.0991014117932136, 0.10089858745503334, 0.10218228468999017, 0.10295250307151686, 0.10680359434982013, 0.11270860059806104, 0.12092426154901188, 0.1347881897805744, 0.1491655963720498, 0.1661103981641505, 0.17766367189767884, 0.18664955123015636, 0.19152759982318412, 0.19512195097849214, 0.19563543037517386, 0.19897304181492528]}
