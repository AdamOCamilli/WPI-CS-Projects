"D:\CS4341 Intro to Artificial Intelligence\python projects\venv\Scripts\python.exe" "D:/CS4341 Intro to Artificial Intelligence/project1/Project_2/template.py"
Using TensorFlow backend.
Train on 3895 samples, validate on 978 samples
Epoch 1/25
2018-02-24 12:42:47.161102: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2

 512/3895 [==>...........................] - ETA: 1s - loss: 11.5929 - acc: 0.1035
3895/3895 [==============================] - 0s 68us/step - loss: 3.8179 - acc: 0.1181 - val_loss: 2.2777 - val_acc: 0.1227
Epoch 2/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2862 - acc: 0.1055
3895/3895 [==============================] - 0s 12us/step - loss: 2.2682 - acc: 0.1191 - val_loss: 2.2562 - val_acc: 0.1299
Epoch 3/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2471 - acc: 0.1270
3895/3895 [==============================] - 0s 13us/step - loss: 2.2486 - acc: 0.1261 - val_loss: 2.2288 - val_acc: 0.1411
Epoch 4/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2011 - acc: 0.1621
3895/3895 [==============================] - 0s 14us/step - loss: 2.2008 - acc: 0.1625 - val_loss: 2.1322 - val_acc: 0.2260
Epoch 5/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1758 - acc: 0.1992
3895/3895 [==============================] - 0s 13us/step - loss: 2.1049 - acc: 0.2275 - val_loss: 2.0643 - val_acc: 0.2413
Epoch 6/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.0213 - acc: 0.2422
3895/3895 [==============================] - 0s 16us/step - loss: 2.0550 - acc: 0.2377 - val_loss: 2.0253 - val_acc: 0.2464
Epoch 7/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.9453 - acc: 0.2812
3895/3895 [==============================] - 0s 14us/step - loss: 2.0258 - acc: 0.2403 - val_loss: 2.0241 - val_acc: 0.2556
Epoch 8/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.9790 - acc: 0.2871
3895/3895 [==============================] - 0s 13us/step - loss: 2.0080 - acc: 0.2467 - val_loss: 1.9833 - val_acc: 0.2587
Epoch 9/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.9579 - acc: 0.2422
3895/3895 [==============================] - 0s 13us/step - loss: 1.9827 - acc: 0.2552 - val_loss: 1.9693 - val_acc: 0.2658
Epoch 10/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.9713 - acc: 0.2441
3895/3895 [==============================] - 0s 14us/step - loss: 1.9502 - acc: 0.2711 - val_loss: 1.9221 - val_acc: 0.2894
Epoch 11/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.9404 - acc: 0.2988
3895/3895 [==============================] - 0s 13us/step - loss: 1.9013 - acc: 0.2909 - val_loss: 1.8683 - val_acc: 0.3098
Epoch 12/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.8520 - acc: 0.3008
3895/3895 [==============================] - 0s 14us/step - loss: 1.8605 - acc: 0.3019 - val_loss: 1.8479 - val_acc: 0.3088
Epoch 13/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.8728 - acc: 0.2871
3895/3895 [==============================] - 0s 13us/step - loss: 1.8359 - acc: 0.3073 - val_loss: 1.8340 - val_acc: 0.3129
Epoch 14/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.8211 - acc: 0.3105
3895/3895 [==============================] - 0s 14us/step - loss: 1.8167 - acc: 0.3122 - val_loss: 1.8259 - val_acc: 0.3037
Epoch 15/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.8427 - acc: 0.2891
3895/3895 [==============================] - 0s 15us/step - loss: 1.8088 - acc: 0.3086 - val_loss: 1.8200 - val_acc: 0.3057
Epoch 16/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.8087 - acc: 0.3047
3895/3895 [==============================] - 0s 13us/step - loss: 1.7972 - acc: 0.3089 - val_loss: 1.8225 - val_acc: 0.3067
Epoch 17/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.7623 - acc: 0.3379
3895/3895 [==============================] - 0s 13us/step - loss: 1.7912 - acc: 0.3125 - val_loss: 1.8051 - val_acc: 0.3057
Epoch 18/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.7387 - acc: 0.2988
3895/3895 [==============================] - 0s 15us/step - loss: 1.7738 - acc: 0.3119 - val_loss: 1.7967 - val_acc: 0.3139
Epoch 19/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.7411 - acc: 0.3223
3895/3895 [==============================] - 0s 14us/step - loss: 1.7641 - acc: 0.3250 - val_loss: 1.7869 - val_acc: 0.3374
Epoch 20/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.7766 - acc: 0.3262
3895/3895 [==============================] - 0s 13us/step - loss: 1.7515 - acc: 0.3646 - val_loss: 1.7806 - val_acc: 0.3640
Epoch 21/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.7428 - acc: 0.3652
3895/3895 [==============================] - 0s 12us/step - loss: 1.7428 - acc: 0.3792 - val_loss: 1.7708 - val_acc: 0.3681
Epoch 22/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.7216 - acc: 0.3984
3895/3895 [==============================] - 0s 14us/step - loss: 1.7318 - acc: 0.3815 - val_loss: 1.7635 - val_acc: 0.3681
Epoch 23/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.7324 - acc: 0.3691
3895/3895 [==============================] - 0s 14us/step - loss: 1.7234 - acc: 0.3813 - val_loss: 1.7548 - val_acc: 0.3701
Epoch 24/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.7540 - acc: 0.3926
3895/3895 [==============================] - 0s 13us/step - loss: 1.7145 - acc: 0.3833 - val_loss: 1.7507 - val_acc: 0.3701
Epoch 25/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.6323 - acc: 0.4199
3895/3895 [==============================] - 0s 13us/step - loss: 1.7079 - acc: 0.3859 - val_loss: 1.7427 - val_acc: 0.3722
{'val_loss': [2.2777446047660033, 2.2561881942007926, 2.2287510791926786, 2.1322101576440415, 2.0642736991734103, 2.0253142308603764, 2.0240710047368866, 1.983278079754492, 1.9693405881737396, 1.9221429093483766, 1.868345977582327, 1.8479472232986325, 1.8339797360765422, 1.8259367959874784, 1.8199859435816728, 1.822516808236791, 1.8050782578123128, 1.796704043152386, 1.7869032236208457, 1.7806330521901448, 1.7708314359797535, 1.7634703237830007, 1.75477431985742, 1.750728276609643, 1.7426656177438842], 'val_acc': [0.1226993854365222, 0.12985684964920113, 0.14110429207118255, 0.2259713686499859, 0.24130879016497872, 0.24642126947824208, 0.2556237190169547, 0.2586912090122578, 0.2658486732249367, 0.28936605252980696, 0.3098159533885359, 0.3087934536253748, 0.3128834380511126, 0.3036809800714559, 0.305725969846507, 0.30674846872595923, 0.305725969846507, 0.3139059284896207, 0.33742331751528937, 0.36400818587080835, 0.36809815920447525, 0.36809815920447525, 0.3701431458713087, 0.37014315074694426, 0.3721881374137777], 'loss': [3.8179010623838843, 2.2681819979431386, 2.248618994261089, 2.200755040238543, 2.1048521344254656, 2.0549710180701277, 2.0258468011530435, 2.007985805700006, 1.9826630915542622, 1.9501840779349802, 1.9012912620781934, 1.8605405716596122, 1.835908383551856, 1.816738899467233, 1.8088026963394321, 1.797234862698517, 1.7912449854788333, 1.773797834822394, 1.7640862119672234, 1.7514748226540386, 1.742789118586823, 1.7318107360440744, 1.7233529116590154, 1.7145385866752791, 1.7078566109567919], 'acc': [0.11810012853803623, 0.11912708614160222, 0.126059049551539, 0.1625160459376422, 0.22747111711866283, 0.2377406927334941, 0.24030808682848767, 0.24672657237202886, 0.2551989730041048, 0.271116815627922, 0.29088575207988165, 0.30192554485201073, 0.3073170729105831, 0.3121951225250768, 0.30860076995807983, 0.30885751036991665, 0.31245186155965604, 0.3119383814322627, 0.32503209270929034, 0.3645699617033286, 0.37920410896296375, 0.3815147619651432, 0.3812580241547929, 0.38331193798466734, 0.38587933184246653]}
