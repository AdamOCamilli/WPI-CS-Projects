"D:\CS4341 Intro to Artificial Intelligence\python projects\venv\Scripts\python.exe" "D:/CS4341 Intro to Artificial Intelligence/project1/Project_2/template.py"
Using TensorFlow backend.
Train on 3895 samples, validate on 978 samples
Epoch 1/25
2018-02-26 10:51:57.426699: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2

 512/3895 [==>...........................] - ETA: 2s - loss: 2.3030 - acc: 0.1074
3895/3895 [==============================] - 0s 125us/step - loss: 2.3030 - acc: 0.1022 - val_loss: 2.3026 - val_acc: 0.1043
Epoch 2/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3043 - acc: 0.1016
3895/3895 [==============================] - 0s 17us/step - loss: 2.3024 - acc: 0.1071 - val_loss: 2.3020 - val_acc: 0.1155
Epoch 3/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3024 - acc: 0.1172
3895/3895 [==============================] - 0s 17us/step - loss: 2.3018 - acc: 0.1171 - val_loss: 2.3014 - val_acc: 0.1217
Epoch 4/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3022 - acc: 0.1348
3895/3895 [==============================] - 0s 18us/step - loss: 2.3013 - acc: 0.1397 - val_loss: 2.3009 - val_acc: 0.1370
Epoch 5/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3001 - acc: 0.1582
3584/3895 [==========================>...] - ETA: 0s - loss: 2.3007 - acc: 0.1543
3895/3895 [==============================] - 0s 18us/step - loss: 2.3007 - acc: 0.1551 - val_loss: 2.3003 - val_acc: 0.1564
Epoch 6/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.3007 - acc: 0.1465
3895/3895 [==============================] - 0s 17us/step - loss: 2.3001 - acc: 0.1646 - val_loss: 2.2997 - val_acc: 0.1708
Epoch 7/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2991 - acc: 0.1582
3895/3895 [==============================] - 0s 18us/step - loss: 2.2994 - acc: 0.1623 - val_loss: 2.2990 - val_acc: 0.1748
Epoch 8/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2986 - acc: 0.1641
3072/3895 [======================>.......] - ETA: 0s - loss: 2.2988 - acc: 0.1654
3895/3895 [==============================] - 0s 22us/step - loss: 2.2986 - acc: 0.1687 - val_loss: 2.2982 - val_acc: 0.1779
Epoch 9/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2981 - acc: 0.1719
3895/3895 [==============================] - 0s 18us/step - loss: 2.2977 - acc: 0.1715 - val_loss: 2.2972 - val_acc: 0.1769
Epoch 10/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2969 - acc: 0.1484
3584/3895 [==========================>...] - ETA: 0s - loss: 2.2965 - acc: 0.1724
3895/3895 [==============================] - 0s 18us/step - loss: 2.2964 - acc: 0.1746 - val_loss: 2.2959 - val_acc: 0.1728
Epoch 11/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2927 - acc: 0.2090
3895/3895 [==============================] - 0s 16us/step - loss: 2.2947 - acc: 0.1684 - val_loss: 2.2940 - val_acc: 0.1687
Epoch 12/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2925 - acc: 0.1641
3895/3895 [==============================] - 0s 18us/step - loss: 2.2924 - acc: 0.1705 - val_loss: 2.2915 - val_acc: 0.1677
Epoch 13/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2915 - acc: 0.1543
3895/3895 [==============================] - 0s 19us/step - loss: 2.2893 - acc: 0.1700 - val_loss: 2.2877 - val_acc: 0.1595
Epoch 14/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2866 - acc: 0.1738
3895/3895 [==============================] - 0s 17us/step - loss: 2.2845 - acc: 0.1638 - val_loss: 2.2819 - val_acc: 0.1544
Epoch 15/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2769 - acc: 0.1680
3895/3895 [==============================] - 0s 16us/step - loss: 2.2773 - acc: 0.1623 - val_loss: 2.2729 - val_acc: 0.1534
Epoch 16/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2617 - acc: 0.1699
3584/3895 [==========================>...] - ETA: 0s - loss: 2.2663 - acc: 0.1613
3895/3895 [==============================] - 0s 18us/step - loss: 2.2664 - acc: 0.1620 - val_loss: 2.2603 - val_acc: 0.1554
Epoch 17/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2571 - acc: 0.1523
3895/3895 [==============================] - 0s 19us/step - loss: 2.2516 - acc: 0.1697 - val_loss: 2.2433 - val_acc: 0.1595
Epoch 18/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2498 - acc: 0.1504
3895/3895 [==============================] - 0s 17us/step - loss: 2.2334 - acc: 0.1782 - val_loss: 2.2241 - val_acc: 0.1759
Epoch 19/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2000 - acc: 0.2148
3895/3895 [==============================] - 0s 18us/step - loss: 2.2132 - acc: 0.1882 - val_loss: 2.2037 - val_acc: 0.1902
Epoch 20/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.2082 - acc: 0.1953
3895/3895 [==============================] - 0s 18us/step - loss: 2.1916 - acc: 0.1933 - val_loss: 2.1828 - val_acc: 0.1973
Epoch 21/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1962 - acc: 0.1895
3895/3895 [==============================] - 0s 15us/step - loss: 2.1697 - acc: 0.2005 - val_loss: 2.1616 - val_acc: 0.2014
Epoch 22/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1278 - acc: 0.2109
3895/3895 [==============================] - 0s 16us/step - loss: 2.1470 - acc: 0.2026 - val_loss: 2.1414 - val_acc: 0.2045
Epoch 23/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1355 - acc: 0.1934
3895/3895 [==============================] - 0s 19us/step - loss: 2.1253 - acc: 0.2033 - val_loss: 2.1243 - val_acc: 0.2035
Epoch 24/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1204 - acc: 0.2109
3895/3895 [==============================] - 0s 17us/step - loss: 2.1076 - acc: 0.2080 - val_loss: 2.1090 - val_acc: 0.2106
Epoch 25/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1310 - acc: 0.2188
3895/3895 [==============================] - 0s 15us/step - loss: 2.0916 - acc: 0.2123 - val_loss: 2.0969 - val_acc: 0.2147
{'val_loss': [2.302576107481506, 2.301995190618472, 2.3013917746475627, 2.3008666267668056, 2.3003361278760166, 2.2997267060972186, 2.299031711063502, 2.2982177032283477, 2.297178643856556, 2.2958549058754265, 2.2940208946025202, 2.2914792272454636, 2.2877429867570873, 2.2818890978222246, 2.272901242496046, 2.2602596565745845, 2.243283038246607, 2.224149066733924, 2.2037296573077243, 2.1828149710696167, 2.1615880232890934, 2.1414317129092226, 2.1243486053373184, 2.1090402052202597, 2.0969269748601933], 'val_acc': [0.10429447852189373, 0.11554192227705673, 0.12167689160030556, 0.1370143149055708, 0.15644171821041097, 0.17075664707000515, 0.17484662574020754, 0.17791411084844777, 0.17689161551906774, 0.17280163597658368, 0.1687116564340996, 0.16768916155038313, 0.15950920245398773, 0.15439672801445536, 0.15337423312883436, 0.1554192229038855, 0.15950920245398773, 0.17586912065439672, 0.1901840490797546, 0.19734151329243355, 0.20143149284253578, 0.20449897750511248, 0.20347648261568238, 0.21063394682074313, 0.21472392636513174], 'loss': [2.303013707003024, 2.3024495452298126, 2.301846207580762, 2.301267587113289, 2.300707933869074, 2.300104416442009, 2.2994115124311922, 2.298621933083785, 2.297652710264546, 2.2964010597958646, 2.2947124821536806, 2.29243272829117, 2.289271259124227, 2.284514823223109, 2.2773174801901157, 2.26644864106821, 2.2516492612248666, 2.2333665381959222, 2.2131948700612254, 2.1916293659626445, 2.169713052093447, 2.146998517901968, 2.125348353906223, 2.107568341378835, 2.0915527876901687], 'acc': [0.1021822847072059, 0.10706033360629087, 0.1170731709000388, 0.1396662391387452, 0.15507060386173532, 0.16456996170715435, 0.16225930717468873, 0.16867779198751828, 0.17150192557617697, 0.17458279849016928, 0.16842105245942177, 0.17047496766655373, 0.16996148884755502, 0.1637997430884333, 0.1622593067156029, 0.16200256761216078, 0.16970475005017013, 0.1781771505292175, 0.18818998715920382, 0.1933247748059394, 0.2005134791900931, 0.20256739354791628, 0.2033376123540974, 0.20795892104410849, 0.21232349134608625]}
