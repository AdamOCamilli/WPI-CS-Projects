"D:\CS4341 Intro to Artificial Intelligence\python projects\venv\Scripts\python.exe" "D:/CS4341 Intro to Artificial Intelligence/project1/Project_2/template.py"
Using TensorFlow backend.
Train on 3895 samples, validate on 978 samples
Epoch 1/25
2018-02-26 10:36:46.993460: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2

 512/3895 [==>...........................] - ETA: 1s - loss: 2.3173 - acc: 0.0918
3895/3895 [==============================] - 0s 82us/step - loss: 2.2563 - acc: 0.1209 - val_loss: 2.1718 - val_acc: 0.1728
Epoch 2/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1767 - acc: 0.1465
3895/3895 [==============================] - 0s 14us/step - loss: 2.1045 - acc: 0.1843 - val_loss: 1.9876 - val_acc: 0.2260
Epoch 3/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.0115 - acc: 0.2188
3895/3895 [==============================] - 0s 15us/step - loss: 1.9214 - acc: 0.2899 - val_loss: 1.8099 - val_acc: 0.3231
Epoch 4/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.8035 - acc: 0.3320
3895/3895 [==============================] - 0s 14us/step - loss: 1.7914 - acc: 0.3420 - val_loss: 1.6344 - val_acc: 0.4059
Epoch 5/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.6415 - acc: 0.3750
3895/3895 [==============================] - 0s 14us/step - loss: 1.6790 - acc: 0.3746 - val_loss: 1.5584 - val_acc: 0.4100
Epoch 6/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.5442 - acc: 0.3984
3895/3895 [==============================] - 0s 14us/step - loss: 1.5080 - acc: 0.4293 - val_loss: 1.7185 - val_acc: 0.4172
Epoch 7/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.6080 - acc: 0.4316
3895/3895 [==============================] - 0s 15us/step - loss: 1.5111 - acc: 0.4190 - val_loss: 1.3865 - val_acc: 0.4939
Epoch 8/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.3956 - acc: 0.4902
3895/3895 [==============================] - 0s 15us/step - loss: 1.3944 - acc: 0.4937 - val_loss: 1.2494 - val_acc: 0.5245
Epoch 9/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.2735 - acc: 0.5234
3895/3895 [==============================] - 0s 15us/step - loss: 1.1960 - acc: 0.5813 - val_loss: 1.3329 - val_acc: 0.5225
Epoch 10/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.3630 - acc: 0.5352
3895/3895 [==============================] - 0s 15us/step - loss: 1.1535 - acc: 0.6126 - val_loss: 1.0942 - val_acc: 0.6063
Epoch 11/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.0992 - acc: 0.6191
3895/3895 [==============================] - 0s 14us/step - loss: 1.1254 - acc: 0.6144 - val_loss: 0.9304 - val_acc: 0.6656
Epoch 12/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.9040 - acc: 0.7031
3895/3895 [==============================] - 0s 15us/step - loss: 0.9025 - acc: 0.7094 - val_loss: 0.8802 - val_acc: 0.6963
Epoch 13/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.8756 - acc: 0.7246
3895/3895 [==============================] - 0s 15us/step - loss: 0.9259 - acc: 0.6917 - val_loss: 0.8871 - val_acc: 0.6718
Epoch 14/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.8933 - acc: 0.6855
3895/3895 [==============================] - 0s 16us/step - loss: 0.8451 - acc: 0.7153 - val_loss: 0.7353 - val_acc: 0.7495
Epoch 15/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.7404 - acc: 0.7715
3895/3895 [==============================] - 0s 16us/step - loss: 0.8860 - acc: 0.7019 - val_loss: 0.8467 - val_acc: 0.7157
Epoch 16/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.7382 - acc: 0.7812
3895/3895 [==============================] - 0s 16us/step - loss: 0.6809 - acc: 0.7928 - val_loss: 0.7425 - val_acc: 0.7720
Epoch 17/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.7814 - acc: 0.7676
3895/3895 [==============================] - 0s 15us/step - loss: 0.7184 - acc: 0.7697 - val_loss: 0.6450 - val_acc: 0.7863
Epoch 18/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.5965 - acc: 0.7988
3895/3895 [==============================] - 0s 14us/step - loss: 0.5977 - acc: 0.8095 - val_loss: 0.7942 - val_acc: 0.7515
Epoch 19/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.6112 - acc: 0.7852
3895/3895 [==============================] - 0s 13us/step - loss: 0.6990 - acc: 0.7815 - val_loss: 0.6999 - val_acc: 0.7740
Epoch 20/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.6787 - acc: 0.7637
3895/3895 [==============================] - 0s 15us/step - loss: 0.6461 - acc: 0.7902 - val_loss: 0.6307 - val_acc: 0.8078
Epoch 21/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.5835 - acc: 0.8223
3895/3895 [==============================] - 0s 15us/step - loss: 0.5551 - acc: 0.8282 - val_loss: 0.5939 - val_acc: 0.8160
Epoch 22/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.4790 - acc: 0.8477
3895/3895 [==============================] - 0s 15us/step - loss: 0.5057 - acc: 0.8472 - val_loss: 0.5554 - val_acc: 0.8282
Epoch 23/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.4500 - acc: 0.8652
3895/3895 [==============================] - 0s 14us/step - loss: 0.5593 - acc: 0.8285 - val_loss: 0.6202 - val_acc: 0.8231
Epoch 24/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.5928 - acc: 0.8262
3895/3895 [==============================] - 0s 14us/step - loss: 0.4971 - acc: 0.8531 - val_loss: 0.5735 - val_acc: 0.8292
Epoch 25/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.5875 - acc: 0.8164
3895/3895 [==============================] - 0s 14us/step - loss: 0.4945 - acc: 0.8470 - val_loss: 0.5488 - val_acc: 0.8364
{'val_loss': [2.1718348106236056, 1.9875998277605678, 1.809854161276164, 1.6344395246973797, 1.558399182887165, 1.7184669264498909, 1.3865029360862842, 1.2494319233670068, 1.3328736077057073, 1.0942317242515112, 0.9303723415226537, 0.880204750228757, 0.8870655166590872, 0.7352665426297178, 0.8466986332446763, 0.7425455188946246, 0.6449926680826215, 0.7941910676917172, 0.6998527459571698, 0.6307307275770144, 0.5939393264140576, 0.5553729406163737, 0.6202081663720691, 0.5734810038090727, 0.5488443740306457], 'val_acc': [0.17280163404156582, 0.22597137215434895, 0.32310838781007717, 0.40593047150561173, 0.41002044703331464, 0.4171779166091927, 0.4938650357333185, 0.524539882724758, 0.5224948889273076, 0.6063394802479656, 0.6656441801896125, 0.6963190285218518, 0.6717791476864025, 0.7494887435363115, 0.7157464082255686, 0.7719836310618738, 0.7862985736265748, 0.7515337373337619, 0.7740286194961251, 0.8077709676054114, 0.8159509191483808, 0.8282208599927235, 0.823108370928189, 0.82924336597232, 0.8364008275033994], 'loss': [2.256250787668877, 2.104484554096119, 1.9214023910835865, 1.7913961398280172, 1.6789599680013008, 1.5079713294992705, 1.5111002061119128, 1.3944074202257188, 1.196002442203223, 1.1534916697784934, 1.125375553058874, 0.902516301115915, 0.9258646095059191, 0.8450856809744632, 0.8859520949968478, 0.6809320512264776, 0.7183827422855754, 0.5976933939007035, 0.6990451480672051, 0.6460863829577229, 0.5550822361442947, 0.5057270363757485, 0.5592579863573376, 0.4970768324600411, 0.494520993991749], 'acc': [0.12092426224529207, 0.1843388958292534, 0.2898587929115981, 0.34197689327716213, 0.3745827975184376, 0.4292682931343612, 0.4189987166013583, 0.4937098865484549, 0.581258023642147, 0.6125802325498461, 0.614377404728352, 0.7093709862584174, 0.6916559686862764, 0.7152759952936919, 0.7019255443011077, 0.7928112941926801, 0.7697047485772697, 0.8094993575087561, 0.7815147646278411, 0.7902439010311612, 0.8282413330249517, 0.8472400494809328, 0.8284980762754692, 0.8531450597404362, 0.8469833120761083]}
