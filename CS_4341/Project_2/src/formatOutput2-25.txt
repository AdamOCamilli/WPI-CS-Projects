"D:\CS4341 Intro to Artificial Intelligence\python projects\venv\Scripts\python.exe" "D:/CS4341 Intro to Artificial Intelligence/project1/Project_2/template.py"
Using TensorFlow backend.
2018-02-24 12:37:29.542964: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
Train on 3895 samples, validate on 978 samples
Epoch 1/25

 512/3895 [==>...........................] - ETA: 1s - loss: 2.4342 - acc: 0.0781
3895/3895 [==============================] - 0s 75us/step - loss: 2.2641 - acc: 0.1345 - val_loss: 2.1701 - val_acc: 0.1933
Epoch 2/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1448 - acc: 0.2305
3895/3895 [==============================] - 0s 14us/step - loss: 2.1187 - acc: 0.2303 - val_loss: 2.0626 - val_acc: 0.2474
Epoch 3/25

 512/3895 [==>...........................] - ETA: 0s - loss: 2.0715 - acc: 0.2500
3895/3895 [==============================] - 0s 12us/step - loss: 2.0125 - acc: 0.2675 - val_loss: 1.9321 - val_acc: 0.3098
Epoch 4/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.9103 - acc: 0.3164
3895/3895 [==============================] - 0s 14us/step - loss: 1.8990 - acc: 0.3083 - val_loss: 1.8638 - val_acc: 0.3221
Epoch 5/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.9070 - acc: 0.3086
3895/3895 [==============================] - 0s 14us/step - loss: 1.7982 - acc: 0.3412 - val_loss: 1.7980 - val_acc: 0.3650
Epoch 6/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.7559 - acc: 0.3926
3895/3895 [==============================] - 0s 14us/step - loss: 1.7152 - acc: 0.3725 - val_loss: 1.7287 - val_acc: 0.3742
Epoch 7/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.6890 - acc: 0.3730
3895/3895 [==============================] - 0s 14us/step - loss: 1.6480 - acc: 0.3987 - val_loss: 1.6622 - val_acc: 0.4018
Epoch 8/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.6913 - acc: 0.3574
3895/3895 [==============================] - 0s 14us/step - loss: 1.5963 - acc: 0.4085 - val_loss: 1.6108 - val_acc: 0.4417
Epoch 9/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.4965 - acc: 0.4727
3895/3895 [==============================] - 0s 13us/step - loss: 1.5325 - acc: 0.4406 - val_loss: 1.5716 - val_acc: 0.4325
Epoch 10/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.4965 - acc: 0.4434
3895/3895 [==============================] - 0s 14us/step - loss: 1.4564 - acc: 0.4606 - val_loss: 1.4816 - val_acc: 0.4550
Epoch 11/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.3752 - acc: 0.4824
3895/3895 [==============================] - 0s 14us/step - loss: 1.3692 - acc: 0.4863 - val_loss: 1.4797 - val_acc: 0.4775
Epoch 12/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.3633 - acc: 0.4941
3895/3895 [==============================] - 0s 14us/step - loss: 1.3644 - acc: 0.5191 - val_loss: 1.4019 - val_acc: 0.5348
Epoch 13/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.3127 - acc: 0.5820
3895/3895 [==============================] - 0s 14us/step - loss: 1.2284 - acc: 0.6198 - val_loss: 1.2568 - val_acc: 0.5951
Epoch 14/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.1437 - acc: 0.6406
3895/3895 [==============================] - 0s 14us/step - loss: 1.1594 - acc: 0.6306 - val_loss: 1.3572 - val_acc: 0.5532
Epoch 15/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.2380 - acc: 0.5703
3895/3895 [==============================] - 0s 13us/step - loss: 1.1623 - acc: 0.6139 - val_loss: 1.1122 - val_acc: 0.6452
Epoch 16/25

 512/3895 [==>...........................] - ETA: 0s - loss: 1.0177 - acc: 0.6660
3895/3895 [==============================] - 0s 14us/step - loss: 0.9505 - acc: 0.7078 - val_loss: 1.1177 - val_acc: 0.6391
Epoch 17/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.9660 - acc: 0.6699
3895/3895 [==============================] - 0s 13us/step - loss: 0.9743 - acc: 0.6945 - val_loss: 1.0225 - val_acc: 0.6871
Epoch 18/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.9105 - acc: 0.7129
3895/3895 [==============================] - 0s 15us/step - loss: 0.8672 - acc: 0.7240 - val_loss: 0.8908 - val_acc: 0.7280
Epoch 19/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.7395 - acc: 0.7734
3895/3895 [==============================] - 0s 13us/step - loss: 0.7658 - acc: 0.7710 - val_loss: 0.8438 - val_acc: 0.7423
Epoch 20/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.7937 - acc: 0.7422
3895/3895 [==============================] - 0s 13us/step - loss: 0.7464 - acc: 0.7707 - val_loss: 0.8737 - val_acc: 0.7311
Epoch 21/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.8480 - acc: 0.7598
3895/3895 [==============================] - 0s 15us/step - loss: 0.7736 - acc: 0.7612 - val_loss: 0.8084 - val_acc: 0.7444
Epoch 22/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.6795 - acc: 0.7812
3895/3895 [==============================] - 0s 14us/step - loss: 0.6817 - acc: 0.7954 - val_loss: 0.7526 - val_acc: 0.7669
Epoch 23/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.6192 - acc: 0.8301
3895/3895 [==============================] - 0s 14us/step - loss: 0.6491 - acc: 0.8031 - val_loss: 0.7212 - val_acc: 0.7832
Epoch 24/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.5917 - acc: 0.8320
3895/3895 [==============================] - 0s 14us/step - loss: 0.6054 - acc: 0.8223 - val_loss: 0.7182 - val_acc: 0.7853
Epoch 25/25

 512/3895 [==>...........................] - ETA: 0s - loss: 0.6150 - acc: 0.8164
3895/3895 [==============================] - 0s 14us/step - loss: 0.6805 - acc: 0.7949 - val_loss: 0.7224 - val_acc: 0.7894
{'val_loss': [2.170117577648358, 2.062615707608088, 1.9320583616541451, 1.8638199623864609, 1.7979952998931665, 1.728699561520832, 1.6621746029590536, 1.6108204891832816, 1.5716257999767311, 1.4816300405802658, 1.4796960085691362, 1.4019455241766932, 1.2568171745916574, 1.3572112368172176, 1.1121555542409542, 1.1176550476097622, 1.0225039101329563, 0.8907540763570243, 0.8438348333772219, 0.8737348986062048, 0.8084021859130002, 0.7525641258752663, 0.721243741078367, 0.7182184696685073, 0.7224200293574109], 'val_acc': [0.19325153352902225, 0.24744376259834977, 0.309815956892899, 0.3220858919474245, 0.36503067630931646, 0.3742331254214109, 0.40184049317441833, 0.4417177848289349, 0.43251533535116776, 0.45501022769142024, 0.4775051133886193, 0.5347648239818331, 0.5950920213707142, 0.5531697217184586, 0.6451942659838312, 0.6390592983651503, 0.6871165691709226, 0.7280163557739102, 0.7423313015077743, 0.7310838431180864, 0.7443762770215914, 0.7668711783208243, 0.7832310903047979, 0.7852760738634137, 0.7893660631647871], 'loss': [2.2640674700020824, 2.118716535274422, 2.0125335454022624, 1.8989589378065568, 1.7982345884663455, 1.7152453040586968, 1.6480261603736144, 1.5962509117016284, 1.5325243858685083, 1.4564288067113755, 1.3692460521041812, 1.3643866309152488, 1.228352659076108, 1.1593640296847891, 1.1623491705611673, 0.9504756656323532, 0.9742755501658987, 0.8672306538393317, 0.7657690525514017, 0.7464256261218344, 0.7735763695640956, 0.6816880700830011, 0.649106741731984, 0.6053738091815268, 0.6804775930491583], 'acc': [0.13453145094875807, 0.23029524921146682, 0.2675224645070454, 0.30834403100766633, 0.3412066752016927, 0.37252888232660847, 0.3987163037788577, 0.40847240042166166, 0.44056482599696695, 0.46059049986905404, 0.4862644438260021, 0.5191270840948445, 0.6197689360158282, 0.630551988138926, 0.6138639262154105, 0.707830553474108, 0.6944801035833298, 0.7240051358287846, 0.7709884466959668, 0.7707317068273816, 0.7612323471303164, 0.7953786901393199, 0.8030808730364152, 0.8223363275246381, 0.7948652111672927]}
