"D:\CS4341 Intro to Artificial Intelligence\python projects\venv\Scripts\python.exe" "D:/CS4341 Intro to Artificial Intelligence/project1/Project_2/template.py"
Using TensorFlow backend.
Train on 3895 samples, validate on 978 samples
Epoch 1/20
2018-02-24 12:36:32.589811: I C:\tf_jenkins\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2

 512/3895 [==>...........................] - ETA: 1s - loss: 2.3244 - acc: 0.1055
3895/3895 [==============================] - 0s 76us/step - loss: 2.2514 - acc: 0.1653 - val_loss: 2.1621 - val_acc: 0.2536
Epoch 2/20

 512/3895 [==>...........................] - ETA: 0s - loss: 2.1669 - acc: 0.2363
3895/3895 [==============================] - 0s 13us/step - loss: 2.1069 - acc: 0.2570 - val_loss: 2.0029 - val_acc: 0.2791
Epoch 3/20

 512/3895 [==>...........................] - ETA: 0s - loss: 1.9845 - acc: 0.2656
3895/3895 [==============================] - 0s 14us/step - loss: 1.9659 - acc: 0.2953 - val_loss: 1.8672 - val_acc: 0.3282
Epoch 4/20

 512/3895 [==>...........................] - ETA: 0s - loss: 1.9198 - acc: 0.3184
3895/3895 [==============================] - 0s 14us/step - loss: 1.8940 - acc: 0.3258 - val_loss: 1.7142 - val_acc: 0.4080
Epoch 5/20

 512/3895 [==>...........................] - ETA: 0s - loss: 1.7952 - acc: 0.3730
3895/3895 [==============================] - 0s 14us/step - loss: 1.7139 - acc: 0.4036 - val_loss: 1.7857 - val_acc: 0.3476
Epoch 6/20

 512/3895 [==>...........................] - ETA: 0s - loss: 1.8428 - acc: 0.3457
3895/3895 [==============================] - 0s 13us/step - loss: 1.6383 - acc: 0.4398 - val_loss: 1.4893 - val_acc: 0.4857
Epoch 7/20

 512/3895 [==>...........................] - ETA: 0s - loss: 1.5549 - acc: 0.4766
3895/3895 [==============================] - 0s 14us/step - loss: 1.5530 - acc: 0.4701 - val_loss: 1.3980 - val_acc: 0.5389
Epoch 8/20

 512/3895 [==>...........................] - ETA: 0s - loss: 1.3870 - acc: 0.5430
3895/3895 [==============================] - 0s 14us/step - loss: 1.4041 - acc: 0.5335 - val_loss: 1.3863 - val_acc: 0.5399
Epoch 9/20

 512/3895 [==>...........................] - ETA: 0s - loss: 1.4335 - acc: 0.5000
3895/3895 [==============================] - 0s 14us/step - loss: 1.3293 - acc: 0.5687 - val_loss: 1.2240 - val_acc: 0.6022
Epoch 10/20

 512/3895 [==>...........................] - ETA: 0s - loss: 1.3154 - acc: 0.5742
3895/3895 [==============================] - 0s 13us/step - loss: 1.2350 - acc: 0.6036 - val_loss: 1.1035 - val_acc: 0.6431
Epoch 11/20

 512/3895 [==>...........................] - ETA: 0s - loss: 1.0710 - acc: 0.6523
3895/3895 [==============================] - 0s 13us/step - loss: 1.1277 - acc: 0.6375 - val_loss: 1.1878 - val_acc: 0.6125
Epoch 12/20

 512/3895 [==>...........................] - ETA: 0s - loss: 1.2196 - acc: 0.6133
3895/3895 [==============================] - 0s 13us/step - loss: 1.1376 - acc: 0.6483 - val_loss: 1.0437 - val_acc: 0.6616
Epoch 13/20

 512/3895 [==>...........................] - ETA: 0s - loss: 1.1559 - acc: 0.6465
3895/3895 [==============================] - 0s 14us/step - loss: 1.0743 - acc: 0.6652 - val_loss: 1.0367 - val_acc: 0.6595
Epoch 14/20

 512/3895 [==>...........................] - ETA: 0s - loss: 1.0762 - acc: 0.6309
3895/3895 [==============================] - 0s 13us/step - loss: 1.0193 - acc: 0.6863 - val_loss: 0.9324 - val_acc: 0.7188
Epoch 15/20

 512/3895 [==>...........................] - ETA: 0s - loss: 0.9808 - acc: 0.6973
3895/3895 [==============================] - 0s 13us/step - loss: 1.0132 - acc: 0.6919 - val_loss: 1.1028 - val_acc: 0.6820
Epoch 16/20

 512/3895 [==>...........................] - ETA: 0s - loss: 0.9903 - acc: 0.7266
3895/3895 [==============================] - 0s 16us/step - loss: 1.0304 - acc: 0.7089 - val_loss: 0.8870 - val_acc: 0.7587
Epoch 17/20

 512/3895 [==>...........................] - ETA: 0s - loss: 0.9216 - acc: 0.7715
3895/3895 [==============================] - 0s 14us/step - loss: 0.8801 - acc: 0.7484 - val_loss: 0.8637 - val_acc: 0.7454
Epoch 18/20

 512/3895 [==>...........................] - ETA: 0s - loss: 0.8171 - acc: 0.7734
3895/3895 [==============================] - 0s 14us/step - loss: 0.8788 - acc: 0.7533 - val_loss: 0.8890 - val_acc: 0.7720
Epoch 19/20

 512/3895 [==>...........................] - ETA: 0s - loss: 0.8038 - acc: 0.7930
3895/3895 [==============================] - 0s 13us/step - loss: 0.9600 - acc: 0.7379 - val_loss: 0.8559 - val_acc: 0.7699
Epoch 20/20

 512/3895 [==>...........................] - ETA: 0s - loss: 0.8741 - acc: 0.7773
3895/3895 [==============================] - 0s 13us/step - loss: 0.8365 - acc: 0.7764 - val_loss: 0.8671 - val_acc: 0.7720
{'val_loss': [2.16206193262814, 2.002859229690458, 1.8672345769429743, 1.7142398059977588, 1.7856985216004229, 1.4893338858228031, 1.397988790383368, 1.386334650355614, 1.2240305900086166, 1.1034723005899378, 1.1878243364195638, 1.0436606759430196, 1.0367014499529739, 0.9324074517973605, 1.1027836433948914, 0.8870015440543005, 0.8637467867757646, 0.8889748646681538, 0.8558757375841979, 0.8671302120134631], 'val_acc': [0.253578730948376, 0.27914111020618665, 0.3282208581034147, 0.4079754649983349, 0.34764826492785433, 0.4856850630422799, 0.5388548163304787, 0.5398772880587354, 0.6022494881431018, 0.64314928717896, 0.612474437505921, 0.6615541991768196, 0.6595092053793691, 0.718813915072287, 0.6820040849820237, 0.7586912005713137, 0.7453987781255523, 0.7719836389847816, 0.7699386505505302, 0.771983652758452], 'loss': [2.251404445131569, 2.1068781145117863, 1.965889782624006, 1.8940178684459876, 1.7139371411638173, 1.6382527509917038, 1.5530360249407944, 1.4041384066176505, 1.3292935405983393, 1.2349620223504434, 1.1276603336358713, 1.137647105885409, 1.0742734032509722, 1.0192608834996304, 1.0132373711876141, 1.0303737730973799, 0.8800556121037768, 0.8787794554952786, 0.9600441699309588, 0.836530334300652], 'acc': [0.1653401798323581, 0.2569961481054573, 0.2952503212762277, 0.3258023099507539, 0.4035943527965399, 0.4397946087019434, 0.4700898564213813, 0.5335044941026837, 0.5686777933571244, 0.6035943506005793, 0.637483953266609, 0.6482670093990566, 0.6652118119563003, 0.6862644438566078, 0.6919127100851478, 0.708857507745476, 0.7483953797312236, 0.7532734255888649, 0.7378690628705497, 0.776379975213625]}
